{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4f9d23-d9d0-40da-9484-738ae08d5684",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a837b6e-df9a-401d-90ed-f4684e1418b3",
   "metadata": {},
   "source": [
    "# Part 3: VSS Event Review\n",
    "\n",
    "## Event Review \n",
    "\n",
    "In this notebook we'll explore the [NVIDIA AI blueprint for video search and summarization (VSS)](https://build.nvidia.com/nvidia/video-search-and-summarization/blueprintcard) Event Review feature introduced in VSS 2.4 which allows VSS to act as an intelligent add on to any Computer Vision Pipeline by allowing for direct VLM access. \n",
    "\n",
    "### Learning Objectives:\n",
    "This notebook explores the following topics:\n",
    "* VSS REST APIs for direct VLM Access \n",
    "* Use VSS to generate video captions \n",
    "* Analyze short video clips for summaries, Q&A and Alerts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a0a100-dc91-416f-a3a8-af5ee0081e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vss_url = \"http://localhost:8100\"\n",
    "warehouse_safety_video = \"assets/warehouse_safety_video.mp4\"\n",
    "warehouse_safety_video_short_ppe = \"assets/warehouse_safety_video_short_ppe.mp4\"\n",
    "warehouse_safety_video_short_no_ppe = \"assets/warehouse_safety_video_short_no_ppe.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9f4a32-71ee-4341-9175-563d6c87ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_endpoint = vss_url + \"/health/ready\" #check the status of the VSS server\n",
    "upload_file_endpoint = vss_url + \"/files\" #upload and manage files\n",
    "summarize_endpoint = vss_url + \"/summarize\" #summarize uploaded content\n",
    "qna_endpoint = vss_url + \"/chat/completions\" #ask questions for ingested video\n",
    "review_alert_endpoint = vss_url + \"/reviewAlert\"\n",
    "generate_vlm_captions_endpoint = vss_url + \"/generate_vlm_captions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05375bf-54c4-4735-bf96-dfb076a15474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m ensurepip --upgrade\n",
    "!{python_exe} -m pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03fe3be-e9a3-40d7-b53a-6219d6d9f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to verify responses \n",
    "import json\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def check_response(response, text=False):\n",
    "    print(f\"Response Code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Response Status: Success\")\n",
    "        if text:\n",
    "            print(response.text)\n",
    "            return response.text\n",
    "        else:\n",
    "            print(json.dumps(response.json(), indent=4))\n",
    "            return response.json()\n",
    "    else:\n",
    "        print(\"Response Status: Error\")\n",
    "        print(response.text)\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c78af-0aff-498b-b544-f4197ab3a357",
   "metadata": {},
   "source": [
    "Let's use the health endpoint to verify your VSS instance is running. **Make sure the following cell outputs \"Response Code: 200\" before proceeding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14ad57-cf8c-4287-a0d2-b0c3a2506bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(health_endpoint)\n",
    "resp = check_response(resp, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56bdd6-af8a-461b-b353-68a508324f42",
   "metadata": {},
   "source": [
    "Then lets save the configured VLM model so we can use it in future requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d84d8a-09fd-4c77-aefa-976ba4bc0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = requests.get(vss_url + \"/models\")\n",
    "    resp = check_response(resp)\n",
    "    configured_vlm = resp[\"data\"][0][\"id\"]\n",
    "except Exception as e:\n",
    "    print(f'Server not ready: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd08c9-fc92-4362-b8d0-b260bb38ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configured VLM: {configured_vlm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ec410-2fec-4926-a64c-c7002ab87b65",
   "metadata": {},
   "source": [
    "### Part 1: Direct VLM Access\n",
    "<img alt=\"event reviewer apis\" src=\"assets/event_review_apis.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b376e05-1b6b-4476-b6c0-2f62e03e819c",
   "metadata": {},
   "source": [
    "VSS 2.4 introduced a new Event Review feature that allows direct access to the VLM through the `/generate_vlm_captions` endpoints and a convenient `/reviewAlert` endpoint to generate boolean states and descriptions from the VLM over short video clips. \n",
    "\n",
    "The intention of the review alert endpoint is to provide a way to answer yes/no questions with direct VLM calls, bypassing the need for full ingestion and LLM needed when using the `/chat/completion` or `/summarize` endpoint. \n",
    "\n",
    "This is best suited when low latency responses are required on short clips of video. \n",
    "\n",
    "The `/generate_vlm_captions` endpoint allows direct access to the VLM for the purpose of generating captions across an input video and receiving the captions in the request response. This can be done for both short and long videos. It can also be used for general Q&A on video clips to the VLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88952177",
   "metadata": {},
   "source": [
    "For this notebook we will use a warehouse ladder safety example. In this two minute video, two workers climb a green ladder in the center aisle of the warehouse. One worker wears proper PPE (Hardhat & Safety Vest) while the other does not. \n",
    "\n",
    "<video width=\"1000 \" height=\" \" \n",
    "       src=\"assets/warehouse_safety_video.mp4\"  \n",
    "       controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58731b19",
   "metadata": {},
   "source": [
    "The next cell will upload the warehouse ladder safety video to VSS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c90379",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warehouse_safety_video, \"rb\") as file:\n",
    "    files = {\"file\": (\"warehouse_safety_video.mp4\", file)} #provide the file content along with a file name \n",
    "    data = {\"purpose\":\"vision\", \"media_type\":\"video\"}\n",
    "    response = requests.post(upload_file_endpoint, data=data, files=files) #post file upload request \n",
    "response = check_response(response)\n",
    "video_id = response[\"id\"] #save file ID for summarization request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ff6d7",
   "metadata": {},
   "source": [
    "#### Part 1.1 VLM Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab3a23-1058-4b32-ba88-2b62bf2914a8",
   "metadata": {},
   "source": [
    "The `/generate_vlm_captions` endpoint is similar to the `/summarize` endpoint however, it does not trigger the summarization or RAG pipelines that are used to generate summaries and enable Q&A on videos. The `/generate_vlm_captions` endpoint will only make calls to the VLM and directly return the VLM captions generated on the video. It will not generate summaries or store the captions into a database. \n",
    "\n",
    "This endpoint does not require the deployment of an LLM, embedding or reranker models, which makes it more suitable for lightweight single GPU and edge GPU deployments. \n",
    "\n",
    "If your application does not need to summarize or produce a database of the video captions, then you can call the `/generate_vlm_captions` endpoint to take advantage of the GPU optimized decoding and frame selection pipeline to have a high throughput VLM captioning pipeline. \n",
    "\n",
    "The `/generate_vlm_captions` endpoint is also suitable if you intend to build your own database connectors and agent to operate on top of the VLM generated captions. \n",
    "\n",
    "Using this endpoint is a great debugging tool if your summaries or Q&A through the `/summarize` and `/chat_completions` endpoint are not providing good accuracy. You can use this endpoint to easily inspect, tweak and tune your VLM prompts to ensure the VLM is producing descriptions with the relevant data for summarization and Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed186004",
   "metadata": {},
   "source": [
    "The following cell shows the body of the request used for the `/generate_vlm_captions` endpoint. It is nearly identical to the `/summarize` endpoint body but with fewer parameters because we are only configuring prompts and parameters for the VLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc0545b-ecd0-4022-9039-63e3a8ed9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"id\": video_id, #id of file returned after upload \n",
    "    \"model\": configured_vlm,\n",
    "    \"system_prompt\": \"Your task is to accurately caption an input video.\", #VLM System Prompt \n",
    "    \"prompt\": \"Write a detailed dense caption describing the events in video.\", #VLM User Prompt\n",
    "    \"max_tokens\": 512, #max tokens for VLM \n",
    "    \"temperature\": 0.4, #temperature for VLM \n",
    "    \"top_p\": 0.4, #top p for VLM \n",
    "    \"chunk_duration\": 20,\n",
    "    \"enable_reasoning\": False,\n",
    "    \"chunk_overlap_duration\": 0,\n",
    "    \"num_frames_per_chunk\" : 20,\n",
    "    \"vlm_input_width\": 1280,\n",
    "    \"vlm_input_height\": 720\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1237db",
   "metadata": {},
   "source": [
    "With the body defined, we can post the request to the `/generate_vlm_captions` endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(generate_vlm_captions_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69508a",
   "metadata": {},
   "source": [
    "From the request response, we can extract the VLM caption for each chunk in the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b757a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_responses = response[\"chunk_responses\"]\n",
    "for chunk_response in chunk_responses:\n",
    "    print(f\"Time: {chunk_response['start_time']} - {chunk_response['end_time']}\\n\")\n",
    "    print(f\"Reasoning: {chunk_response['reasoning_description']}\\n\")\n",
    "    print(f\"Caption: {chunk_response[\"content\"]}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c434f",
   "metadata": {},
   "source": [
    "Scroll through the response and see how the VLM generates the captions over each video chunk. Try adjusting the system and user prompts to the VLM to see how the captions change.\n",
    "\n",
    "In the next few cells, lets enable VLM reasoning and generate the captions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ec1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"id\": video_id, #id of file returned after upload \n",
    "    \"model\": configured_vlm,\n",
    "    \"system_prompt\": \"Your task is to accurately caption an input video.\", #VLM System Prompt \n",
    "    \"prompt\": \"Write a detailed dense caption describing the events in video.\", #User Prompt for VLM \n",
    "    \"max_tokens\": 1024, #max tokens for VLM \n",
    "    \"temperature\": 0.4, #temperature for VLM \n",
    "    \"top_p\": 0.4, #top p for VLM \n",
    "    \"chunk_duration\": 20,\n",
    "    \"enable_reasoning\": True,\n",
    "    \"chunk_overlap_duration\": 0,\n",
    "    \"num_frames_per_chunk\" : 20,\n",
    "    \"vlm_input_width\": 1280,\n",
    "    \"vlm_input_height\": 720\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4192c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(generate_vlm_captions_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7747d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_responses = response[\"chunk_responses\"]\n",
    "for chunk_response in chunk_responses:\n",
    "    print(f\"Time: {chunk_response['start_time']} - {chunk_response['end_time']}\\n\")\n",
    "    print(f\"Reasoning: {chunk_response['reasoning_description']}\\n\")\n",
    "    print(f\"Caption: {chunk_response[\"content\"]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71822865",
   "metadata": {},
   "source": [
    "Notice how with reasoning enabled, the additional `reasoning_description` field is populated. This is the reasoning trace generated by the VLM prior to generating the text in the `content` field. \n",
    "\n",
    "This reasoning trace can help improve the VLM's ability to understand the input frames to provide more detailed captions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbef0a1",
   "metadata": {},
   "source": [
    "#### Part 1.2 Analyze Short Clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048ef30",
   "metadata": {},
   "source": [
    "Another way to use the `/generate_vlm_captions` endpoint is for direct Q&A to the VLM on short video clips. \n",
    "\n",
    "For this warehouse safety example, it would not be very efficient to continously run a VLM to always check if someone is wearing PPE while on the ladder. If deployed, there would be a significant amount of time where no one is on the ladder that the VLM would be called on. \n",
    "\n",
    "A great use of VSS with direct access to the VLM is to combine it with a lightweight computer vision pipeline such as a DeepStream detection pipeline to run continuosly and detect when someone is on the ladder.\n",
    "\n",
    "These short clips output by the computer vision pipeline can then be sent to the VLM to check for PPE and trigger an alert or notification. By combining a light weight computer vision pipeline with VSS, you can build an efficient pipeline suitable for edge deployments to gain insights that can't be answered with standard computer vision models. \n",
    "\n",
    "Lets take the two 30 second clips of when the person with and without PPE are present on the ladder and analyze them with VSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1626a",
   "metadata": {},
   "source": [
    "![no ppe](assets/warehouse_ladder_no_ppe.png)\n",
    "![no ppe](assets/warehouse_ladder_ppe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30cbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warehouse_safety_video_short_ppe, \"rb\") as file:\n",
    "    files = {\"file\": (\"warehouse_safety_video_short_ppe.mp4\", file)} #provide the file content along with a file name \n",
    "    data = {\"purpose\":\"vision\", \"media_type\":\"video\"}\n",
    "    response = requests.post(upload_file_endpoint, data=data, files=files) #post file upload request \n",
    "response = check_response(response)\n",
    "video_id_ppe = response[\"id\"] #save file ID for summarization request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warehouse_safety_video_short_no_ppe, \"rb\") as file:\n",
    "    files = {\"file\": (\"warehouse_safety_video_short_no_ppe.mp4\", file)} #provide the file content along with a file name \n",
    "    data = {\"purpose\":\"vision\", \"media_type\":\"video\"}\n",
    "    response = requests.post(upload_file_endpoint, data=data, files=files) #post file upload request \n",
    "response = check_response(response)\n",
    "video_id_no_ppe = response[\"id\"] #save file ID for summarization request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7451dca",
   "metadata": {},
   "source": [
    "We can define a simple wrapper around the /generate_vlm_captions endpoint for direct VLM Q&A to analyze the clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87516d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vlm_qna(video_id, question, enable_reasoning=False):\n",
    "    body = {\n",
    "        \"id\": video_id, #id of file returned after upload \n",
    "        \"model\": configured_vlm,\n",
    "        \"system_prompt\": \"Your task is to view the video and answer the user's question accurately based on the video.\", #VLM System Prompt \n",
    "        \"prompt\": question, #User Prompt for VLM \n",
    "        \"max_tokens\": 1024, #max tokens for VLM \n",
    "        \"temperature\": 0.4, #temperature for VLM \n",
    "        \"top_p\": 0.4, #top p for VLM \n",
    "        \"chunk_duration\": 120,\n",
    "        \"enable_reasoning\": enable_reasoning,\n",
    "        \"chunk_overlap_duration\": 0,\n",
    "        \"num_frames_per_chunk\" : 20,\n",
    "        \"vlm_input_width\": 1280,\n",
    "        \"vlm_input_height\": 720\n",
    "    }\n",
    "    response = requests.post(generate_vlm_captions_endpoint, json=body)\n",
    "    response = check_response(response)\n",
    "    return response[\"chunk_responses\"][0][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43185b",
   "metadata": {},
   "source": [
    "In the body of the request, we will set the chunk duration to a value larger than the video length to guarntee only 1 VLM call is made for the entire video. \n",
    "\n",
    "Lets try some questions on both of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_ppe, \"Does anyone use a ladder?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea838",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_no_ppe, \"Does anyone use a ladder?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_ppe, \"Does the person the ladder have a hardhat and safety vest?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_no_ppe, \"Does the person the ladder have a hardhat and safety vest?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e9a4c",
   "metadata": {},
   "source": [
    "Lets try a more open ended question and see how the VLM is able to reason through it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f042b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_ppe, \"Is the person using the ladder safely?\", enable_reasoning=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_no_ppe, \"Is the person using the ladder safely?\", enable_reasoning=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef72e16",
   "metadata": {},
   "source": [
    "For short video clips, this is a good way to do low latency Q&A. However for long video clips, accuracy may degrade because the VLM can only take in a certain number of frames at once. As the length of the input video grows, the VLM will view fewer frames of the video and may miss important events to answer the user's question. \n",
    "\n",
    "For this reason, Q&A on long videos is recommended to use the /summarize and /chat_completions endpoint to take advantage of the LLM and GraphRAG for long video understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0db32",
   "metadata": {},
   "source": [
    "### Part 2: Event Review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2c09c",
   "metadata": {},
   "source": [
    "In the previous section, we saw how to directly access the VLM to generate captions and perform Q&A on short video clips. \n",
    "\n",
    "An alternative way to directly access the VLM to analyze a video is to use the `/reviewAlert` endpoint. This endpoint provides additional parameters and return values that make it more suitable to connect with a computer vision pipeline and video management system for building a full event review workflow. \n",
    "\n",
    "Often times, a light weight computer vision pipeline based on detection data and hueristic based algorithms can try to determine if certain events or states in a video are True or False. For example, a computer vision pipeline can try to determine if two cars from a traffic camera have had a collision or if a person has walked through a door. However, traditional computer vision pipelines may have a high rate of false positives because detection data and heuristic based algorithms to determine if an event has occured is not able to fully understand the context of  the video. \n",
    "\n",
    "For this reason, a Vision Language Model can be used as a judge to evaluate the output of a computer vision pipeline to determine if a detected event should be true or false as well as provide further insights through a natural language description that cannot be derived from traditional computer vision models and pipelines. \n",
    "\n",
    "The endpoint uses the VLM to determine the answer to a yes/no question over a video clip and returns a boolean state that is based on the VLM's response. This boolean state can then be integrated with a notification system, dashboard or other applications to trigger alerts when the VLM determines the answer to a question is True. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabb747",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at the body of a `/reviewAlert` request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "829cd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = f\"{video_id_ppe}/warehouse_safety_video_short_ppe.mp4\" #local path of uploaded video file \n",
    "\n",
    "body = {\n",
    "    \"version\": \"1.0\", #api version \n",
    "    \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", #unique ID of event provided by the client \n",
    "    \"@timestamp\": \"2024-05-30T01:41:25.000Z\", #timestamp of the event \n",
    "    \"sensor_id\": \"camera-01\", #ID of camera that detected the event \n",
    "    \"video_path\": local_path, #path to video file to inspect \n",
    "    \"confidence\": 0.5, #optional confidence score for the true/false state from the computer vision pipeline \n",
    "    \"alert\": { #optional alert metadata can be added to prioritize the alerts and define alert types \n",
    "            \"severity\": \"CRITICAL\",\n",
    "            \"status\": \"REVIEW_PENDING\",\n",
    "            \"type\": \"Ladder Safety\",\n",
    "            \"description\": \"Verify PPE usage on ladder\"\n",
    "            },\n",
    "    \n",
    "    \"event\": {\"type\": \"Ladder Safety Alert\", \"description\": \"Worker present on ladder\"},\n",
    "    \"vss_params\":\n",
    "    {\n",
    "        \"chunk_duration\": 60,\n",
    "        \"num_frames_per_chunk\": 10,\n",
    "        \"do_verification\": True,\n",
    "        \"enable_reasoning\": False,\n",
    "        \"vlm_params\":\n",
    "        {   \n",
    "            \"system_prompt\": \"You are a warehouse monitoring system tasked with identifying safety events in a warehouse. Answer the user's question accurately based on the input video.\",\n",
    "            \"prompt\": \"Is the person on the ladder wearing a hardhat and safety vest?\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "\n",
    "\n",
    "    }\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fafe64",
   "metadata": {},
   "source": [
    "The `/reviewAlert` endpoint was built to support a local computer vision pipeline outputting short video clips. For this reason, there is a video_path parameter that accepts a local file path to access a video file to use an input to the VLM call. \n",
    "\n",
    "Video files uploaded through the `/files` endpoint can also be used by constructing the local path of the video file using the returned filed ID and file name. \n",
    "\n",
    "There are additonal parameters such as timestamp, sensor ID, alert severity, alert descriptions and more to attach extra metadata to the event. This metadata can be used by an external system to track the alerts.\n",
    "\n",
    "The `vss_params` section should look very familiar to the /summarize and /generate_vlm_captions endpoint as it contains the standard parameters for chunk duration, VLM prompts and parameters. \n",
    "\n",
    "Lets submit the request and view the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ad5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(review_alert_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e96f5",
   "metadata": {},
   "source": [
    "From the request response, we can see it returned the metadata that was provided in the request body along with a new `result` field that contains the VLM results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d89e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response[\"result\"]\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de330425",
   "metadata": {},
   "source": [
    "In the results, the description field is the response from the VLM. \n",
    "The verification_result is the boolean state determined by the VLMs response to our question. \n",
    "\n",
    "In this case, we asked the VLM to verify the person on the ladder is wearing PPE which it verified as true. \n",
    "\n",
    "Lets try this again but on the 30 second clip of the worker without PPE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcfac6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = f\"{video_id_no_ppe}/warehouse_safety_video_short_no_ppe.mp4\"\n",
    "body = {\n",
    "    \"version\": \"1.0\", #api version \n",
    "    \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", #unique ID of event provided by the client \n",
    "    \"@timestamp\": \"2024-05-30T01:41:25.000Z\", #timestamp of the event \n",
    "    \"sensor_id\": \"camera-01\", #ID of camera that detected the event \n",
    "    \"video_path\": local_path, #path to video file to inspect \n",
    "    \"confidence\": 0.5, #optional confidence score for the true/false state from the computer vision pipeline \n",
    "    \"alert\": { #optional alert metadata can be added to prioritize the alerts and define alert types \n",
    "            \"severity\": \"CRITICAL\",\n",
    "            \"status\": \"REVIEW_PENDING\",\n",
    "            \"type\": \"Ladder Safety\",\n",
    "            \"description\": \"Verify PPE usage on ladder\"\n",
    "            },\n",
    "    \n",
    "    \"event\": {\"type\": \"Ladder Safety Alert\", \"description\": \"Worker present on ladder\"},\n",
    "    \"vss_params\":\n",
    "    {\n",
    "        \"chunk_duration\": 60,\n",
    "        \"num_frames_per_chunk\": 10,\n",
    "        \"do_verification\": True,\n",
    "        \"enable_reasoning\": False,\n",
    "        \"vlm_params\":\n",
    "        {   \n",
    "            \"system_prompt\": \"You are a warehouse monitoring system tasked with identifying safety events in a warehouse. Answer the user's question accurately based on the input video.\",\n",
    "            \"prompt\": \"Is the person on the ladder wearing a hardhat and safety vest?\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "\n",
    "\n",
    "    }\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(review_alert_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c564c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response[\"result\"]\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed31de",
   "metadata": {},
   "source": [
    "With this video clip, we can see the description from the VLM does not include PPE and the verification result is False. \n",
    "\n",
    "For a full reference workflow combining the VSS Event Review features with a computer vision pipeline, visit the following page:\n",
    "- https://docs.nvidia.com/vss/latest/content/vss_event_reviewer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c823e87",
   "metadata": {},
   "source": [
    "---\n",
    "### Review\n",
    "\n",
    "In this notebook you learned the following:\n",
    "- How to directly access the VLM in VSS \n",
    "- Generate VLM captions on a video \n",
    "- Analyze short video clips with direct VLM access \n",
    "- Generate boolean states and descriptions on short video clips "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
