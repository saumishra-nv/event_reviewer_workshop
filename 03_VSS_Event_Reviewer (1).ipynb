{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4f9d23-d9d0-40da-9484-738ae08d5684",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d23797-c8c8-42ef-b990-e48e9f0405c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Disk with largest free space: /ephemeral (734.00 GB free)\n",
      "‚û°Ô∏è Docker will use data-root: /ephemeral/docker\n",
      "üìù Writing /etc/docker/daemon.json with: {'data-root': '/ephemeral/docker'}\n",
      " Restart Docker services...\n",
      " Docker Root Dir: /ephemeral/docker\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "###############################################################\n",
    "# CELL 1: Configure Docker to use the disk with MOST free space\n",
    "#\n",
    "# What this does:\n",
    "#   1. Reads `df -h` to get all mounted filesystems\n",
    "#   2. Filters out things like tmpfs, /run, /dev, etc.\n",
    "#   3. Picks the mountpoint with the largest free space (in GB)\n",
    "#   4. Writes /etc/docker/daemon.json with:\n",
    "#        { \"data-root\": \"<best_mount>/docker\" }\n",
    "#   5. Stops Docker, deletes old /var/lib/docker (destructive),\n",
    "#      creates new <best_mount>/docker, restarts Docker\n",
    "#   6. Prints Docker Root Dir for verification\n",
    "###############################################################\n",
    "\n",
    "def get_mounts_with_free_space():\n",
    "    \"\"\"\n",
    "    Parse `df -h` and return a list of (mountpoint, free_gb).\n",
    "    We skip non-interesting mountpoints like /run, /dev, tmpfs, etc.\n",
    "    \"\"\"\n",
    "    # --output=target,avail so we get mountpoint + free human-readable\n",
    "    out = subprocess.check_output(\"df -h --output=target,avail | tail -n +2\", shell=True).decode().strip().splitlines()\n",
    "    mounts = []\n",
    "    for line in out:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            mount, avail = line.split()\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Filter out virtual / irrelevant mounts\n",
    "        if any(mount.startswith(p) for p in [\"/run\", \"/sys\", \"/proc\", \"/dev\", \"/snap\"]):\n",
    "            continue\n",
    "\n",
    "        # Convert avail string to GB (handles G or M)\n",
    "        if avail.endswith(\"G\"):\n",
    "            free_gb = float(avail[:-1])\n",
    "        elif avail.endswith(\"M\"):\n",
    "            free_gb = float(avail[:-1]) / 1024.0\n",
    "        else:\n",
    "            # Skip weird units (T etc.) for simplicity\n",
    "            continue\n",
    "\n",
    "        mounts.append((mount, free_gb))\n",
    "\n",
    "    if not mounts:\n",
    "        raise RuntimeError(\"No suitable mounts found from df output.\")\n",
    "    return mounts\n",
    "\n",
    "# 1. Discover best disk\n",
    "mounts = get_mounts_with_free_space()\n",
    "best_mount, best_free = sorted(mounts, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "print(f\"üì¶ Disk with largest free space: {best_mount} ({best_free:.2f} GB free)\")\n",
    "\n",
    "docker_root = os.path.join(best_mount, \"docker\")\n",
    "print(\"‚û°Ô∏è Docker will use data-root:\", docker_root)\n",
    "\n",
    "# 2. Prepare daemon.json\n",
    "daemon_cfg = {\"data-root\": docker_root}\n",
    "tmp_daemon = \"/tmp/daemon.json.tmp\"\n",
    "\n",
    "with open(tmp_daemon, \"w\") as f:\n",
    "    json.dump(daemon_cfg, f, indent=2)\n",
    "\n",
    "print(\"üìù Writing /etc/docker/daemon.json with:\", daemon_cfg)\n",
    "subprocess.run(f\"sudo mv {tmp_daemon} /etc/docker/daemon.json\", shell=True, check=False)\n",
    "\n",
    "# 3. Stop Docker + socket\n",
    "print(\" Restart Docker services...\")\n",
    "subprocess.run(\"sudo systemctl restart docker\", shell=True)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify new storage location\n",
    "print(subprocess.run(\"docker info | grep 'Docker Root Dir'\", shell=True, capture_output=True, text=True).stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6588b59f-1efb-4144-a5ae-9aa18bfd2381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VST_BASE_URL: http://localhost:30000\n",
      "ALERT_BRIDGE_BASE_URL: http://localhost:9080\n",
      "ALERT_REVIEW_MEDIA_BASE_DIR: /tmp/alert-media-dir\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# CONFIG + IMPORTS\n",
    "#\n",
    "# This section imports required libraries and declares all the\n",
    "# key URLs and paths needed to talk to the VST and Alert Bridge.\n",
    "#\n",
    "# You only need to edit HOST, STORAGE_HTTP_PORT, and your \n",
    "# alert media base directory if your deployment differs.\n",
    "##############################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "import requests\n",
    "\n",
    "# -----------------------------------------------\n",
    "# HOST where your docker-compose stack is running.\n",
    "# If Jupyter is on the same machine ‚Üí use localhost.\n",
    "# -----------------------------------------------\n",
    "HOST = \"localhost\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PORT for VST's \"storage-ms\" microservice (video registration).\n",
    "# This MUST match STORAGE_HTTP_PORT in your .env file.\n",
    "# Default in NVIDIA examples is usually 32000.\n",
    "# ---------------------------------------------------------------\n",
    "STORAGE_HTTP_PORT = 30000\n",
    "VST_BASE_URL = f\"http://{HOST}:{STORAGE_HTTP_PORT}\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PORT for Alert Bridge.\n",
    "# Alert Bridge exposes a FastAPI server, default port: 9080.\n",
    "# ---------------------------------------------------------------\n",
    "ALERT_BRIDGE_PORT = 9080\n",
    "ALERT_BRIDGE_BASE_URL = f\"http://{HOST}:{ALERT_BRIDGE_PORT}\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# This directory MUST MATCH the docker compose variable:\n",
    "#   ALERT_REVIEW_MEDIA_BASE_DIR\n",
    "# Event Reviewer resolves relative video paths using this folder.\n",
    "#\n",
    "# If you did:\n",
    "#   ALERT_REVIEW_MEDIA_BASE_DIR=/tmp/alert-media-dir docker compose up\n",
    "#\n",
    "# Then set it here exactly the same.\n",
    "# ---------------------------------------------------------------\n",
    "ALERT_REVIEW_MEDIA_BASE_DIR = \"/tmp/alert-media-dir\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# We place user videos under a subfolder inside ALERT_REVIEW_MEDIA_BASE_DIR.\n",
    "# VST receives only *relative paths*, so having a subfolder like \"media/\"\n",
    "# is clean and aligns with official examples.\n",
    "# ---------------------------------------------------------------\n",
    "LOCAL_MEDIA_SUBDIR = \"media\"\n",
    "os.makedirs(os.path.join(ALERT_REVIEW_MEDIA_BASE_DIR, LOCAL_MEDIA_SUBDIR), exist_ok=True)\n",
    "\n",
    "print(\"VST_BASE_URL:\", VST_BASE_URL)\n",
    "print(\"ALERT_BRIDGE_BASE_URL:\", ALERT_BRIDGE_BASE_URL)\n",
    "print(\"ALERT_REVIEW_MEDIA_BASE_DIR:\", ALERT_REVIEW_MEDIA_BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934f9387-6a27-4efd-8b75-601154318dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying local video ‚Üí alert media directory:\n",
      "  Source: ./assets/warehouse_safety_video_short_ppe.mp4\n",
      "  Target: /tmp/alert-media-dir/media/warehouse_safety_video_short_ppe.mp4\n",
      "\n",
      "‚úÖ File copied successfully.\n",
      "Relative path that VST expects: media/warehouse_safety_video_short_ppe.mp4\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# COPY LOCAL VIDEO FILE INTO THE DIRECTORY THAT ALERT BRIDGE \n",
    "# AND VST EXPECT.\n",
    "#\n",
    "# Event Reviewer does NOT fetch files from arbitrary paths.\n",
    "# The file MUST be under ALERT_REVIEW_MEDIA_BASE_DIR.\n",
    "#\n",
    "# You provide the local video file path below.\n",
    "################################################################\n",
    "\n",
    "import shutil\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# EDIT THIS: Full path to any local video on your machine.\n",
    "# Example: \"/home/ubuntu/videos/forklift_incident.mp4\"\n",
    "# ---------------------------------------------------------------\n",
    "SOURCE_VIDEO_PATH = \"./assets/warehouse_safety_video_short_ppe.mp4\"\n",
    "# ./assets/warehouse_safety_video_short_ppe.mp4\n",
    "# ./assets/warehouse_safety_video_short_no_ppe.mp4\n",
    "if not os.path.exists(SOURCE_VIDEO_PATH):\n",
    "    raise FileNotFoundError(f\"Source video not found: {SOURCE_VIDEO_PATH}\")\n",
    "\n",
    "# The filename only (e.g., demo.mp4)\n",
    "target_filename = os.path.basename(SOURCE_VIDEO_PATH)\n",
    "\n",
    "# This is the relative path VST + Alert Bridge will use.\n",
    "relative_media_path = f\"{LOCAL_MEDIA_SUBDIR}/{target_filename}\"\n",
    "\n",
    "# Build absolute target path inside the alert directory:\n",
    "dest_path = os.path.join(ALERT_REVIEW_MEDIA_BASE_DIR, relative_media_path)\n",
    "\n",
    "print(\"Copying local video ‚Üí alert media directory:\")\n",
    "print(\"  Source:\", SOURCE_VIDEO_PATH)\n",
    "print(\"  Target:\", dest_path)\n",
    "\n",
    "shutil.copy2(SOURCE_VIDEO_PATH, dest_path)\n",
    "\n",
    "print(\"\\n‚úÖ File copied successfully.\")\n",
    "print(\"Relative path that VST expects:\", relative_media_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a244b72c-a612-4f1d-8970-57c19908ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending payload to VST (as form data):\n",
      "{\n",
      "  \"mediaFilePath\": \"media/warehouse_safety_video_short_ppe.mp4\",\n",
      "  \"metaDataFilePath\": \"\",\n",
      "  \"metadata\": {\n",
      "    \"sensorId\": \"camera-001\",\n",
      "    \"timestamp\": 1765396539,\n",
      "    \"eventInfo\": \"manual_upload_demo\",\n",
      "    \"streamName\": \"manual_demo_stream\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Status: 200\n",
      "Response: {\n",
      "\t\"bytes\" : 8588407,\n",
      "\t\"created_at\" : \"2025-12-10T19:55:40.369Z\",\n",
      "\t\"filePath\" : \"media/warehouse_safety_video_short_ppe.mp4\",\n",
      "\t\"filename\" : \"warehouse_safety_video_short_ppe\",\n",
      "\t\"id\" : \"976250dc-4144-4c17-b8a6-9c9e603592d1\",\n",
      "\t\"sensorId\" : \"camera-001\"\n",
      "}\n",
      "\n",
      "‚úÖ VST registration complete.\n",
      "VST ID: 976250dc-4144-4c17-b8a6-9c9e603592d1\n",
      "VST filePath: media/warehouse_safety_video_short_ppe.mp4\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# REGISTER THE VIDEO WITH VST (Video Storage Toolkit)\n",
    "#\n",
    "# This version matches the example in NVIDIA docs that uses\n",
    "# form-encoded data, not JSON. The key detail:\n",
    "#   - we send \"metadata\" as json.dumps(metadata)\n",
    "#   - we use `data=form_data` instead of `json=...`\n",
    "################################################################\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "metadata = {\n",
    "    \"sensorId\": \"camera-001\",\n",
    "    \"timestamp\": int(time.time()),      # epoch seconds\n",
    "    \"eventInfo\": \"manual_upload_demo\",\n",
    "    \"streamName\": \"manual_demo_stream\"\n",
    "}\n",
    "\n",
    "# NOTE:\n",
    "# - mediaFilePath is the RELATIVE path under ALERT_REVIEW_MEDIA_BASE_DIR\n",
    "# - metaDataFilePath can be \"\" if you don't have a separate CV metadata file\n",
    "form_data = {\n",
    "    \"mediaFilePath\": relative_media_path,   # e.g. \"media/warehouse_safety_video_short_no_ppe.mp4\"\n",
    "    \"metaDataFilePath\": \"\",                 # no separate CV metadata file\n",
    "    \"metadata\": json.dumps(metadata),       # IMPORTANT: JSON STRING, not object\n",
    "}\n",
    "\n",
    "print(\"Sending payload to VST (as form data):\")\n",
    "print(json.dumps({\"mediaFilePath\": form_data[\"mediaFilePath\"],\n",
    "                  \"metaDataFilePath\": form_data[\"metaDataFilePath\"],\n",
    "                  \"metadata\": metadata}, indent=2))\n",
    "\n",
    "resp = requests.post(f\"{VST_BASE_URL}/api/v1/storage/file\", data=form_data)\n",
    "print(\"\\nStatus:\", resp.status_code)\n",
    "print(\"Response:\", resp.text)\n",
    "\n",
    "resp.raise_for_status()\n",
    "vst_response = resp.json()\n",
    "vst_id = vst_response[\"id\"]\n",
    "\n",
    "print(\"\\n‚úÖ VST registration complete.\")\n",
    "print(\"VST ID:\", vst_id)\n",
    "print(\"VST filePath:\", vst_response.get(\"filePath\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325b216d-cac1-4e86-80a9-936b5d90d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'976250dc-4144-4c17-b8a6-9c9e603592d1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vst_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea39c15-dbbb-4aea-8435-97e3813e61bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending alert to Alert Bridge with payload:\n",
      "{'@timestamp': '2025-12-10T20:04:14Z',\n",
      " 'alert': {'description': 'Manual alert triggered from Jupyter.',\n",
      "           'severity': 'MEDIUM',\n",
      "           'status': 'REVIEW_PENDING',\n",
      "           'type': 'manual_demo_alert'},\n",
      " 'confidence': 1.0,\n",
      " 'cv_metadata_path': '',\n",
      " 'end_time': '9999.0',\n",
      " 'event': {'description': 'Manual demo event', 'type': 'video_analysis'},\n",
      " 'id': 'd5a518bb-70f9-416d-9e03-2700ad53b108',\n",
      " 'meta_labels': [{'key': 'prompt_index', 'value': '0'},\n",
      "                 {'key': 'prompt_text',\n",
      "                  'value': 'Is the person on the ladder wearing a hardhat and '\n",
      "                           'safety vest?'},\n",
      "                 {'key': 'enable_reasoning', 'value': 'False'}],\n",
      " 'sensor_id': 'manual_demo_sensor',\n",
      " 'start_time': '0.0',\n",
      " 'version': '1.0',\n",
      " 'video_path': 'media/warehouse_safety_video_short_ppe.mp4',\n",
      " 'vss_params': {'cv_metadata_overlay': False,\n",
      "                'debug': True,\n",
      "                'do_verification': True,\n",
      "                'enable_reasoning': False,\n",
      "                'vlm_params': {'max_tokens': 128,\n",
      "                               'prompt': 'Is the person on the ladder wearing '\n",
      "                                         'a hardhat and safety vest?',\n",
      "                               'seed': 42,\n",
      "                               'system_prompt': 'You are a warehouse '\n",
      "                                                'monitoring system tasked with '\n",
      "                                                'identifying safety events in '\n",
      "                                                'a warehouse. Answer the '\n",
      "                                                \"user's question accurately \"\n",
      "                                                'based on the input video.',\n",
      "                               'temperature': 0.3,\n",
      "                               'top_k': 40,\n",
      "                               'top_p': 0.3}},\n",
      " 'vst_id': '976250dc-4144-4c17-b8a6-9c9e603592d1'}\n",
      "\n",
      "Status: 202\n",
      "Response: {\"status\":\"accepted\",\"id\":\"d5a518bb-70f9-416d-9e03-2700ad53b108\",\"message\":\"Alert queued for processing\",\"timestamp\":\"2025-12-10T20:04:14.787860Z\"}\n",
      "\n",
      "‚úÖ Alert successfully submitted.\n",
      "Alert ID: d5a518bb-70f9-416d-9e03-2700ad53b108\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# SEND ALERT TO ALERT BRIDGE\n",
    "#\n",
    "# This is the main step. You build an alert JSON payload and send:\n",
    "#\n",
    "#   POST /api/v1/alerts\n",
    "#\n",
    "# This triggers:\n",
    "#   1. VLM Reasoning (Cosmos-Reason1-7B) inside Event Reviewer\n",
    "#   2. Event summary generation\n",
    "#   3. Display in the Alert Inspector UI\n",
    "#\n",
    "# Fields to know:\n",
    "#   - id ‚Üí unique alert ID\n",
    "#   - video_path ‚Üí ABSOLUTE PATH to the file\n",
    "#   - vst_id ‚Üí links the alert to VST‚Äôs stored clip\n",
    "#   - start_time / end_time ‚Üí seconds in the clip (use full clip)\n",
    "#   - vss_params ‚Üí how VLM should analyze the clip\n",
    "################################################################\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "alert_id = str(uuid.uuid4())  # unique per alert\n",
    "\n",
    "# ISO timestamp in UTC per spec\n",
    "timestamp_iso = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Absolute file path required by Alert Bridge\n",
    "# video_abs_path = os.path.join(ALERT_REVIEW_MEDIA_BASE_DIR, relative_media_path)\n",
    "\n",
    "# Use RELATIVE path for video_path; backend will join it with ALERT_REVIEW_MEDIA_BASE_DIR\n",
    "video_abs_path = relative_media_path  # e.g. \"media/warehouse_safety_video_short_no_ppe.mp4\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIGURE VLM (Cosmos) CALL\n",
    "# ---------------------------\n",
    "vlm_params = {\n",
    "    \"prompt\": \"Is the person on the ladder wearing a hardhat and safety vest?\",\n",
    "    \"system_prompt\": \"You are a warehouse monitoring system tasked with identifying safety events in a warehouse. Answer the user's question accurately based on the input video.\",\n",
    "    \"max_tokens\": 128,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.3,\n",
    "    \"top_k\": 40,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# VSS pipeline options\n",
    "vss_params = {\n",
    "    \"vlm_params\": vlm_params,\n",
    "    \"cv_metadata_overlay\": False,   # No CV pipeline\n",
    "    \"enable_reasoning\": False,      # No chain-of-thought, keep short\n",
    "    \"do_verification\": True,\n",
    "    \"debug\": True\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# BUILD ALERT PAYLOAD\n",
    "# ---------------------------\n",
    "alert_payload = {\n",
    "    \"id\": alert_id,\n",
    "    \"version\": \"1.0\",\n",
    "    \"@timestamp\": timestamp_iso,\n",
    "    \"sensor_id\": \"manual_demo_sensor\",\n",
    "    \"video_path\": video_abs_path,\n",
    "    \"vst_id\": vst_id,\n",
    "    \"start_time\": \"0.0\",       # full clip\n",
    "    \"end_time\": \"9999.0\",      # full clip\n",
    "    \"alert\": {\n",
    "        \"severity\": \"MEDIUM\",\n",
    "        \"status\": \"REVIEW_PENDING\",\n",
    "        \"type\": \"manual_demo_alert\",\n",
    "        \"description\": \"Manual alert triggered from Jupyter.\"\n",
    "    },\n",
    "    \"event\": {\n",
    "        \"type\": \"video_analysis\",\n",
    "        \"description\": \"Manual demo event\"\n",
    "    },\n",
    "    \"confidence\": 1.0,\n",
    "    \"cv_metadata_path\": \"\",  # Not used\n",
    "    \"vss_params\": vss_params,\n",
    "    \"meta_labels\": [\n",
    "        {\"key\": \"prompt_index\", \"value\": \"0\"},\n",
    "        {\"key\": \"prompt_text\", \"value\": vlm_params[\"prompt\"]},\n",
    "        {\"key\": \"enable_reasoning\", \"value\": \"False\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Sending alert to Alert Bridge with payload:\")\n",
    "pprint(alert_payload)\n",
    "\n",
    "resp = requests.post(f\"{ALERT_BRIDGE_BASE_URL}/api/v1/alerts\", json=alert_payload)\n",
    "print(\"\\nStatus:\", resp.status_code)\n",
    "print(\"Response:\", resp.text)\n",
    "resp.raise_for_status()\n",
    "\n",
    "print(\"\\n‚úÖ Alert successfully submitted.\")\n",
    "print(\"Alert ID:\", alert_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a968f54-0de5-4621-a379-946b19b34335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18948a09-5de6-4c2c-9433-861f8324761e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ad7b0-d474-4648-8301-ea6dd210e7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a11511-be63-4cea-ad29-683d9d8b9875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a682e-3d75-4403-9f1a-e838a73addea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a837b6e-df9a-401d-90ed-f4684e1418b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Part 3: VSS Event Review\n",
    "\n",
    "## Event Review \n",
    "\n",
    "In this notebook we'll explore the [NVIDIA AI blueprint for video search and summarization (VSS)](https://build.nvidia.com/nvidia/video-search-and-summarization/blueprintcard) Event Review feature introduced in VSS 2.4 which allows VSS to act as an intelligent add on to any Computer Vision Pipeline by allowing for direct VLM access. \n",
    "\n",
    "### Learning Objectives:\n",
    "This notebook explores the following topics:\n",
    "* VSS REST APIs for direct VLM Access \n",
    "* Use VSS to generate video captions \n",
    "* Analyze short video clips for summaries, Q&A and Alerts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a0a100-dc91-416f-a3a8-af5ee0081e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vss_url = \"http://localhost:8100\"\n",
    "warehouse_safety_video = \"assets/warehouse_safety_video.mp4\"\n",
    "warehouse_safety_video_short_ppe = \"assets/warehouse_safety_video_short_ppe.mp4\"\n",
    "warehouse_safety_video_short_no_ppe = \"assets/warehouse_safety_video_short_no_ppe.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9f4a32-71ee-4341-9175-563d6c87ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_endpoint = vss_url + \"/health/ready\" #check the status of the VSS server\n",
    "upload_file_endpoint = vss_url + \"/files\" #upload and manage files\n",
    "summarize_endpoint = vss_url + \"/summarize\" #summarize uploaded content\n",
    "qna_endpoint = vss_url + \"/chat/completions\" #ask questions for ingested video\n",
    "review_alert_endpoint = vss_url + \"/reviewAlert\"\n",
    "generate_vlm_captions_endpoint = vss_url + \"/generate_vlm_captions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05375bf-54c4-4735-bf96-dfb076a15474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m ensurepip --upgrade\n",
    "!{python_exe} -m pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03fe3be-e9a3-40d7-b53a-6219d6d9f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to verify responses \n",
    "import json\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def check_response(response, text=False):\n",
    "    print(f\"Response Code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Response Status: Success\")\n",
    "        if text:\n",
    "            print(response.text)\n",
    "            return response.text\n",
    "        else:\n",
    "            print(json.dumps(response.json(), indent=4))\n",
    "            return response.json()\n",
    "    else:\n",
    "        print(\"Response Status: Error\")\n",
    "        print(response.text)\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c78af-0aff-498b-b544-f4197ab3a357",
   "metadata": {},
   "source": [
    "Let's use the health endpoint to verify your VSS instance is running. **Make sure the following cell outputs \"Response Code: 200\" before proceeding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14ad57-cf8c-4287-a0d2-b0c3a2506bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(health_endpoint)\n",
    "resp = check_response(resp, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56bdd6-af8a-461b-b353-68a508324f42",
   "metadata": {},
   "source": [
    "Then lets save the configured VLM model so we can use it in future requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d84d8a-09fd-4c77-aefa-976ba4bc0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = requests.get(vss_url + \"/models\")\n",
    "    resp = check_response(resp)\n",
    "    configured_vlm = resp[\"data\"][0][\"id\"]\n",
    "except Exception as e:\n",
    "    print(f'Server not ready: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd08c9-fc92-4362-b8d0-b260bb38ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configured VLM: {configured_vlm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ec410-2fec-4926-a64c-c7002ab87b65",
   "metadata": {},
   "source": [
    "### Part 1: Direct VLM Access\n",
    "<img alt=\"event reviewer apis\" src=\"assets/event_review_apis.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b376e05-1b6b-4476-b6c0-2f62e03e819c",
   "metadata": {},
   "source": [
    "VSS 2.4 introduced a new Event Review feature that allows direct access to the VLM through the `/generate_vlm_captions` endpoints and a convenient `/reviewAlert` endpoint to generate boolean states and descriptions from the VLM over short video clips. \n",
    "\n",
    "The intention of the review alert endpoint is to provide a way to answer yes/no questions with direct VLM calls, bypassing the need for full ingestion and LLM needed when using the `/chat/completion` or `/summarize` endpoint. \n",
    "\n",
    "This is best suited when low latency responses are required on short clips of video. \n",
    "\n",
    "The `/generate_vlm_captions` endpoint allows direct access to the VLM for the purpose of generating captions across an input video and receiving the captions in the request response. This can be done for both short and long videos. It can also be used for general Q&A on video clips to the VLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88952177",
   "metadata": {},
   "source": [
    "For this notebook we will use a warehouse ladder safety example. In this two minute video, two workers climb a green ladder in the center aisle of the warehouse. One worker wears proper PPE (Hardhat & Safety Vest) while the other does not. \n",
    "\n",
    "<video width=\"1000 \" height=\" \" \n",
    "       src=\"assets/warehouse_safety_video.mp4\"  \n",
    "       controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58731b19",
   "metadata": {},
   "source": [
    "The next cell will upload the warehouse ladder safety video to VSS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c90379",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warehouse_safety_video, \"rb\") as file:\n",
    "    files = {\"file\": (\"warehouse_safety_video.mp4\", file)} #provide the file content along with a file name \n",
    "    data = {\"purpose\":\"vision\", \"media_type\":\"video\"}\n",
    "    response = requests.post(upload_file_endpoint, data=data, files=files) #post file upload request \n",
    "response = check_response(response)\n",
    "video_id = response[\"id\"] #save file ID for summarization request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ff6d7",
   "metadata": {},
   "source": [
    "#### Part 1.1 VLM Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab3a23-1058-4b32-ba88-2b62bf2914a8",
   "metadata": {},
   "source": [
    "The `/generate_vlm_captions` endpoint is similar to the `/summarize` endpoint however, it does not trigger the summarization or RAG pipelines that are used to generate summaries and enable Q&A on videos. The `/generate_vlm_captions` endpoint will only make calls to the VLM and directly return the VLM captions generated on the video. It will not generate summaries or store the captions into a database. \n",
    "\n",
    "This endpoint does not require the deployment of an LLM, embedding or reranker models, which makes it more suitable for lightweight single GPU and edge GPU deployments. \n",
    "\n",
    "If your application does not need to summarize or produce a database of the video captions, then you can call the `/generate_vlm_captions` endpoint to take advantage of the GPU optimized decoding and frame selection pipeline to have a high throughput VLM captioning pipeline. \n",
    "\n",
    "The `/generate_vlm_captions` endpoint is also suitable if you intend to build your own database connectors and agent to operate on top of the VLM generated captions. \n",
    "\n",
    "Using this endpoint is a great debugging tool if your summaries or Q&A through the `/summarize` and `/chat_completions` endpoint are not providing good accuracy. You can use this endpoint to easily inspect, tweak and tune your VLM prompts to ensure the VLM is producing descriptions with the relevant data for summarization and Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed186004",
   "metadata": {},
   "source": [
    "The following cell shows the body of the request used for the `/generate_vlm_captions` endpoint. It is nearly identical to the `/summarize` endpoint body but with fewer parameters because we are only configuring prompts and parameters for the VLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc0545b-ecd0-4022-9039-63e3a8ed9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"id\": video_id, #id of file returned after upload \n",
    "    \"model\": configured_vlm,\n",
    "    \"system_prompt\": \"Your task is to accurately caption an input video.\", #VLM System Prompt \n",
    "    \"prompt\": \"Write a detailed dense caption describing the events in video.\", #VLM User Prompt\n",
    "    \"max_tokens\": 512, #max tokens for VLM \n",
    "    \"temperature\": 0.4, #temperature for VLM \n",
    "    \"top_p\": 0.4, #top p for VLM \n",
    "    \"chunk_duration\": 20,\n",
    "    \"enable_reasoning\": False,\n",
    "    \"chunk_overlap_duration\": 0,\n",
    "    \"num_frames_per_chunk\" : 20,\n",
    "    \"vlm_input_width\": 1280,\n",
    "    \"vlm_input_height\": 720\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1237db",
   "metadata": {},
   "source": [
    "With the body defined, we can post the request to the `/generate_vlm_captions` endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(generate_vlm_captions_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69508a",
   "metadata": {},
   "source": [
    "From the request response, we can extract the VLM caption for each chunk in the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b757a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_responses = response[\"chunk_responses\"]\n",
    "for chunk_response in chunk_responses:\n",
    "    print(f\"Time: {chunk_response['start_time']} - {chunk_response['end_time']}\\n\")\n",
    "    print(f\"Reasoning: {chunk_response['reasoning_description']}\\n\")\n",
    "    print(f\"Caption: {chunk_response[\"content\"]}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c434f",
   "metadata": {},
   "source": [
    "Scroll through the response and see how the VLM generates the captions over each video chunk. Try adjusting the system and user prompts to the VLM to see how the captions change.\n",
    "\n",
    "In the next few cells, lets enable VLM reasoning and generate the captions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ec1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"id\": video_id, #id of file returned after upload \n",
    "    \"model\": configured_vlm,\n",
    "    \"system_prompt\": \"Your task is to accurately caption an input video.\", #VLM System Prompt \n",
    "    \"prompt\": \"Write a detailed dense caption describing the events in video.\", #User Prompt for VLM \n",
    "    \"max_tokens\": 1024, #max tokens for VLM \n",
    "    \"temperature\": 0.4, #temperature for VLM \n",
    "    \"top_p\": 0.4, #top p for VLM \n",
    "    \"chunk_duration\": 20,\n",
    "    \"enable_reasoning\": True,\n",
    "    \"chunk_overlap_duration\": 0,\n",
    "    \"num_frames_per_chunk\" : 20,\n",
    "    \"vlm_input_width\": 1280,\n",
    "    \"vlm_input_height\": 720\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4192c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(generate_vlm_captions_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7747d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_responses = response[\"chunk_responses\"]\n",
    "for chunk_response in chunk_responses:\n",
    "    print(f\"Time: {chunk_response['start_time']} - {chunk_response['end_time']}\\n\")\n",
    "    print(f\"Reasoning: {chunk_response['reasoning_description']}\\n\")\n",
    "    print(f\"Caption: {chunk_response[\"content\"]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71822865",
   "metadata": {},
   "source": [
    "Notice how with reasoning enabled, the additional `reasoning_description` field is populated. This is the reasoning trace generated by the VLM prior to generating the text in the `content` field. \n",
    "\n",
    "This reasoning trace can help improve the VLM's ability to understand the input frames to provide more detailed captions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbef0a1",
   "metadata": {},
   "source": [
    "#### Part 1.2 Analyze Short Clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048ef30",
   "metadata": {},
   "source": [
    "Another way to use the `/generate_vlm_captions` endpoint is for direct Q&A to the VLM on short video clips. \n",
    "\n",
    "For this warehouse safety example, it would not be very efficient to continously run a VLM to always check if someone is wearing PPE while on the ladder. If deployed, there would be a significant amount of time where no one is on the ladder that the VLM would be called on. \n",
    "\n",
    "A great use of VSS with direct access to the VLM is to combine it with a lightweight computer vision pipeline such as a DeepStream detection pipeline to run continuosly and detect when someone is on the ladder.\n",
    "\n",
    "These short clips output by the computer vision pipeline can then be sent to the VLM to check for PPE and trigger an alert or notification. By combining a light weight computer vision pipeline with VSS, you can build an efficient pipeline suitable for edge deployments to gain insights that can't be answered with standard computer vision models. \n",
    "\n",
    "Lets take the two 30 second clips of when the person with and without PPE are present on the ladder and analyze them with VSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1626a",
   "metadata": {},
   "source": [
    "![no ppe](assets/warehouse_ladder_no_ppe.png)\n",
    "![no ppe](assets/warehouse_ladder_ppe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30cbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warehouse_safety_video_short_ppe, \"rb\") as file:\n",
    "    files = {\"file\": (\"warehouse_safety_video_short_ppe.mp4\", file)} #provide the file content along with a file name \n",
    "    data = {\"purpose\":\"vision\", \"media_type\":\"video\"}\n",
    "    response = requests.post(upload_file_endpoint, data=data, files=files) #post file upload request \n",
    "response = check_response(response)\n",
    "video_id_ppe = response[\"id\"] #save file ID for summarization request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warehouse_safety_video_short_no_ppe, \"rb\") as file:\n",
    "    files = {\"file\": (\"warehouse_safety_video_short_no_ppe.mp4\", file)} #provide the file content along with a file name \n",
    "    data = {\"purpose\":\"vision\", \"media_type\":\"video\"}\n",
    "    response = requests.post(upload_file_endpoint, data=data, files=files) #post file upload request \n",
    "response = check_response(response)\n",
    "video_id_no_ppe = response[\"id\"] #save file ID for summarization request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7451dca",
   "metadata": {},
   "source": [
    "We can define a simple wrapper around the /generate_vlm_captions endpoint for direct VLM Q&A to analyze the clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87516d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vlm_qna(video_id, question, enable_reasoning=False):\n",
    "    body = {\n",
    "        \"id\": video_id, #id of file returned after upload \n",
    "        \"model\": configured_vlm,\n",
    "        \"system_prompt\": \"Your task is to view the video and answer the user's question accurately based on the video.\", #VLM System Prompt \n",
    "        \"prompt\": question, #User Prompt for VLM \n",
    "        \"max_tokens\": 1024, #max tokens for VLM \n",
    "        \"temperature\": 0.4, #temperature for VLM \n",
    "        \"top_p\": 0.4, #top p for VLM \n",
    "        \"chunk_duration\": 120,\n",
    "        \"enable_reasoning\": enable_reasoning,\n",
    "        \"chunk_overlap_duration\": 0,\n",
    "        \"num_frames_per_chunk\" : 20,\n",
    "        \"vlm_input_width\": 1280,\n",
    "        \"vlm_input_height\": 720\n",
    "    }\n",
    "    response = requests.post(generate_vlm_captions_endpoint, json=body)\n",
    "    response = check_response(response)\n",
    "    return response[\"chunk_responses\"][0][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43185b",
   "metadata": {},
   "source": [
    "In the body of the request, we will set the chunk duration to a value larger than the video length to guarntee only 1 VLM call is made for the entire video. \n",
    "\n",
    "Lets try some questions on both of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_ppe, \"Does anyone use a ladder?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea838",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_no_ppe, \"Does anyone use a ladder?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_ppe, \"Does the person the ladder have a hardhat and safety vest?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_no_ppe, \"Does the person the ladder have a hardhat and safety vest?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e9a4c",
   "metadata": {},
   "source": [
    "Lets try a more open ended question and see how the VLM is able to reason through it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f042b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_ppe, \"Is the person using the ladder safely?\", enable_reasoning=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = vlm_qna(video_id_no_ppe, \"Is the person using the ladder safely?\", enable_reasoning=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef72e16",
   "metadata": {},
   "source": [
    "For short video clips, this is a good way to do low latency Q&A. However for long video clips, accuracy may degrade because the VLM can only take in a certain number of frames at once. As the length of the input video grows, the VLM will view fewer frames of the video and may miss important events to answer the user's question. \n",
    "\n",
    "For this reason, Q&A on long videos is recommended to use the /summarize and /chat_completions endpoint to take advantage of the LLM and GraphRAG for long video understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0db32",
   "metadata": {},
   "source": [
    "### Part 2: Event Review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2c09c",
   "metadata": {},
   "source": [
    "In the previous section, we saw how to directly access the VLM to generate captions and perform Q&A on short video clips. \n",
    "\n",
    "An alternative way to directly access the VLM to analyze a video is to use the `/reviewAlert` endpoint. This endpoint provides additional parameters and return values that make it more suitable to connect with a computer vision pipeline and video management system for building a full event review workflow. \n",
    "\n",
    "Often times, a light weight computer vision pipeline based on detection data and hueristic based algorithms can try to determine if certain events or states in a video are True or False. For example, a computer vision pipeline can try to determine if two cars from a traffic camera have had a collision or if a person has walked through a door. However, traditional computer vision pipelines may have a high rate of false positives because detection data and heuristic based algorithms to determine if an event has occured is not able to fully understand the context of  the video. \n",
    "\n",
    "For this reason, a Vision Language Model can be used as a judge to evaluate the output of a computer vision pipeline to determine if a detected event should be true or false as well as provide further insights through a natural language description that cannot be derived from traditional computer vision models and pipelines. \n",
    "\n",
    "The endpoint uses the VLM to determine the answer to a yes/no question over a video clip and returns a boolean state that is based on the VLM's response. This boolean state can then be integrated with a notification system, dashboard or other applications to trigger alerts when the VLM determines the answer to a question is True. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabb747",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at the body of a `/reviewAlert` request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "829cd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = f\"{video_id_ppe}/warehouse_safety_video_short_ppe.mp4\" #local path of uploaded video file \n",
    "\n",
    "body = {\n",
    "    \"version\": \"1.0\", #api version \n",
    "    \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", #unique ID of event provided by the client \n",
    "    \"@timestamp\": \"2024-05-30T01:41:25.000Z\", #timestamp of the event \n",
    "    \"sensor_id\": \"camera-01\", #ID of camera that detected the event \n",
    "    \"video_path\": local_path, #path to video file to inspect \n",
    "    \"confidence\": 0.5, #optional confidence score for the true/false state from the computer vision pipeline \n",
    "    \"alert\": { #optional alert metadata can be added to prioritize the alerts and define alert types \n",
    "            \"severity\": \"CRITICAL\",\n",
    "            \"status\": \"REVIEW_PENDING\",\n",
    "            \"type\": \"Ladder Safety\",\n",
    "            \"description\": \"Verify PPE usage on ladder\"\n",
    "            },\n",
    "    \n",
    "    \"event\": {\"type\": \"Ladder Safety Alert\", \"description\": \"Worker present on ladder\"},\n",
    "    \"vss_params\":\n",
    "    {\n",
    "        \"chunk_duration\": 60,\n",
    "        \"num_frames_per_chunk\": 10,\n",
    "        \"do_verification\": True,\n",
    "        \"enable_reasoning\": False,\n",
    "        \"vlm_params\":\n",
    "        {   \n",
    "            \"system_prompt\": \"You are a warehouse monitoring system tasked with identifying safety events in a warehouse. Answer the user's question accurately based on the input video.\",\n",
    "            \"prompt\": \"Is the person on the ladder wearing a hardhat and safety vest?\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "\n",
    "\n",
    "    }\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fafe64",
   "metadata": {},
   "source": [
    "The `/reviewAlert` endpoint was built to support a local computer vision pipeline outputting short video clips. For this reason, there is a video_path parameter that accepts a local file path to access a video file to use an input to the VLM call. \n",
    "\n",
    "Video files uploaded through the `/files` endpoint can also be used by constructing the local path of the video file using the returned filed ID and file name. \n",
    "\n",
    "There are additonal parameters such as timestamp, sensor ID, alert severity, alert descriptions and more to attach extra metadata to the event. This metadata can be used by an external system to track the alerts.\n",
    "\n",
    "The `vss_params` section should look very familiar to the /summarize and /generate_vlm_captions endpoint as it contains the standard parameters for chunk duration, VLM prompts and parameters. \n",
    "\n",
    "Lets submit the request and view the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ad5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(review_alert_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e96f5",
   "metadata": {},
   "source": [
    "From the request response, we can see it returned the metadata that was provided in the request body along with a new `result` field that contains the VLM results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d89e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response[\"result\"]\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de330425",
   "metadata": {},
   "source": [
    "In the results, the description field is the response from the VLM. \n",
    "The verification_result is the boolean state determined by the VLMs response to our question. \n",
    "\n",
    "In this case, we asked the VLM to verify the person on the ladder is wearing PPE which it verified as true. \n",
    "\n",
    "Lets try this again but on the 30 second clip of the worker without PPE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcfac6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = f\"{video_id_no_ppe}/warehouse_safety_video_short_no_ppe.mp4\"\n",
    "body = {\n",
    "    \"version\": \"1.0\", #api version \n",
    "    \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\", #unique ID of event provided by the client \n",
    "    \"@timestamp\": \"2024-05-30T01:41:25.000Z\", #timestamp of the event \n",
    "    \"sensor_id\": \"camera-01\", #ID of camera that detected the event \n",
    "    \"video_path\": local_path, #path to video file to inspect \n",
    "    \"confidence\": 0.5, #optional confidence score for the true/false state from the computer vision pipeline \n",
    "    \"alert\": { #optional alert metadata can be added to prioritize the alerts and define alert types \n",
    "            \"severity\": \"CRITICAL\",\n",
    "            \"status\": \"REVIEW_PENDING\",\n",
    "            \"type\": \"Ladder Safety\",\n",
    "            \"description\": \"Verify PPE usage on ladder\"\n",
    "            },\n",
    "    \n",
    "    \"event\": {\"type\": \"Ladder Safety Alert\", \"description\": \"Worker present on ladder\"},\n",
    "    \"vss_params\":\n",
    "    {\n",
    "        \"chunk_duration\": 60,\n",
    "        \"num_frames_per_chunk\": 10,\n",
    "        \"do_verification\": True,\n",
    "        \"enable_reasoning\": False,\n",
    "        \"vlm_params\":\n",
    "        {   \n",
    "            \"system_prompt\": \"You are a warehouse monitoring system tasked with identifying safety events in a warehouse. Answer the user's question accurately based on the input video.\",\n",
    "            \"prompt\": \"Is the person on the ladder wearing a hardhat and safety vest?\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "\n",
    "\n",
    "    }\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(review_alert_endpoint, json=body)\n",
    "response = check_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c564c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response[\"result\"]\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed31de",
   "metadata": {},
   "source": [
    "With this video clip, we can see the description from the VLM does not include PPE and the verification result is False. \n",
    "\n",
    "For a full reference workflow combining the VSS Event Review features with a computer vision pipeline, visit the following page:\n",
    "- https://docs.nvidia.com/vss/latest/content/vss_event_reviewer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c823e87",
   "metadata": {},
   "source": [
    "---\n",
    "### Review\n",
    "\n",
    "In this notebook you learned the following:\n",
    "- How to directly access the VLM in VSS \n",
    "- Generate VLM captions on a video \n",
    "- Analyze short video clips with direct VLM access \n",
    "- Generate boolean states and descriptions on short video clips "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
